# MongoDB

*[2022.08.04]*

## MongoDB Schemaless

```java
tableA {
	userNumver int,
	userCellPhoneNumber char(13)
}
```

RDB 에서의 DDL 작업은 힘든 작업

→ DB 서버의 3배의 IO를 더 유발하게 된다.

Schemaless는 장점이 아니라 특징일 뿐이다.

embed 환경

Embedded Data Model

일반적으로 Mongodb (document) 데이터 모델

Reference Data Model 

흔히 RDS 데이터 모델

→schemaless 장비가 아무리 좋아도 해결할 수 없는 문제

가장 큰 문제 

1. Array or sub document 
    1. sub document 가 mongoDB 스타일이 맞는데 성능이 좋은 건 아니다. 제일 세부적으로 나눌 수 있는 lock이 document lock. 어떤 값을 변경해도 다른 sessio이 해당 document에 접근 불가

RDS는 contact access가 분리되어 있다…?

1. **동시에 서브 도큐먼트나 어레이 동시성 문제를 개발을 잘하면 되겠지만,** 

업데이트를 하면 기존의 데이터를 새로운 도큐먼트를 넣고 기존 연결 정보를 끊어버림

10 바이트 업데이트하게 되면 ? → 서브 도큐먼트 하나만 수정하는데 생각보다 많은 IO를 유발할 수 있고 동시성 문제가 생길 수 있다.

업데이트가 발생하지 않을 때 도큐먼트 추천하는 이유

만약, 초기 서비스와 달라져서 업데이트 기능이 추가한다면 ? DBA가 해결할 수 없는 문제

업데이트의 가능성이 조금이라도 보인다면 그냥 레퍼런스로 사용하는 것을 추천하곤 한다.

레퍼런스 문제? 이니셜 싱크 (리플리카 셋 부분에서 설명해주시겠다고 함)

룩업 → 조인보다 성능 별로이고 룩업은 못쓴다고 생각하고 접근하는 게 맞음

샤딩 → 청크 마이그레이션할 때 부하가 크다. 

청크 마이그레이션을 줄일 때 큰 장점이 있다.

HBase는 세컨더리 ~ 를 못만듦

Kongodb - Sharding

샤딩 클러스터로 변경할 수 있지만, 주의할 점은 online 중에 할 수는 없다. (readonly 서비스로도 불가능하고 replica set daemon을 내려야 한다.)

그래서 가이드를 할 때 처음부터 최소 구성(최소 샤드 2개)으로 시작해라

Mongodb 

clustered index가 없고 만들수도 없음 → 범위 검색이 빠를 수가 없다.

```java
select ~ from userlog where username='박경선' and logdate between ‘2022-06-10’ AND ‘2022-07-10’;
```

→ 100만건 랜덤 IO를 할 수 밖에 없다.

Nodejs 사용할 때 Mongoose 는 절대 사용하지 말 것.

client driver가 아니라 일종의 edm 툴

오피셜 몽고 드라이버를 사용할 때보다 레이턴시 30~50% 느리다. → 오피셜 몽고 드라이버가 최소 30% 빠르다.

MongoDB storage engine

DB engine → 쿼리 파싱, 옵티마이저 (실행계획), 리플리케이션 , 

storage enigne (Wired Tiger) → IO 처리

3rd-party의 storage enigne를 사용해도 되냐고들 많이 물어보는데 트러블 슈팅이 힘듦 (자료가 거의 없다) 그래서 웬만하면 와이어드 타이거를 사용해라

## MongoDB - Internal

데이터 쓰기

해당 데이터를 (필요 시-update…) 데이터 페이지에서 데이터를 가져와 메모리에 올린 후 메모리에서 작업 후 저널 로그를 기록 → commit ack

실제 데이터 페이지(디스크)는 변경 되지 않았기 때문에 (메모리에만 변경되어있기 때문에) 더티 페이지라고 하고, 싱크를 맞추는 건 체크 포인트라고 한다.

몽고 db의 체크 포인트(싱글스레드)는 굉장히 취약 → 1분에 하나

더티 페이지가 몇 건이건 동시에 내림. 쓰기나 읽기가 밀릴 수 있다.

트래픽이 1000-2000건이라는 작은 QPS 라고 해도 이런 문제가 생긴다.

MVME → 체크 포인트 문제를 조금 완화하고 있다.

그래서 디스크의 랜덤 I/O의 성능이 좋은 서버를 사용해야 한다. (비용은 감안해야 한다.)

메모리 버퍼는 유한하죠. 데이터를 지우는 작업이 필요하고 이것을 eviction

메모리 버퍼에만 데이터를 수정하고 

다른 DB는 디스크 접근 체크포인트와 관련된 임계치를 설정할 수 있는데 몽고는 아직까지 그런 설정이 없다.

체크포인트의 주기를 늘려서 해결될 문제가(그냥 1분에 한 번 발생할 문제를 10초에 한 번 발생하게 된다.) 아니라 디스크 성능을 높여야만 한다.

서버 메모리의 50% 사이즈가 몽고 DB의 버퍼 사이즈. 절대 늘리지 마시구요.

eviction은 캐시 80% 부터 시작된다. foreground eviction이 발생한다고 해서 장애가 발생하는 건 아니다. 

다른 쿼리 처리가 조금 천천해지는 것이고, 물론 크게 발생하기 시작하면 서비스 장애이다.

결론: 체크포인트는 굉장히 heavy하다.

## 데이터 블록 압축

기본이 snappy이고 웬만하면 snappy 추천.

zlib, zstandard 는 압축률이 좋지만 쿼리 성능이 굉장히 떨어짐 (빅데이터 형태로 쿼리 레이턴시가 중요하지 않다면 오히려 추천)

실제 데이터를 압축해서 저장하기 때문에 SQL에 저장하는 것보다 용량이 적을 경우가 빈번

(하지만 메모리 버퍼에 올릴 때에는 압축을 풀어서 올린다.)

디스크에서 읽고 -(압축 풀어서)→ 메모리 버퍼 

→ 스키마리스한 디비는 메모리 용량 측면의 장애를 고려를 하곤 한다.

헤저드 포인트 (다른 스레드에서 사용하고 있는 곳을 참조)

몽고는 IO

Cached I/O: DB 엔진이 하는 게 아니라 OS에 맡긴다. 잘 쓰여졌냐는 몽고의 소관이 아니다. 

OS에서도 캐시가 필요하기 때문에 프리페이지를 얻기 위해 어플리케이션인 몽고의 작업이 느려질 수 있다.

OS의 캐시 메모리를 충분히 둘 수 있도록 구비해야 한다.

### 캐시 이빅션

이빅션할 때 더티 페이지면? 디스크에 저장이 되지 않아 데이터 유실이 되기 때문에 문제를 해결하고자 Look Asid Table(LAT)을 구비해둠

LAT의 사이즈가 커지면 문제가 커진다

LAT 파일은 무조건 존재하긴하는데, 이 파일의 용량이 커지면 (사이즈 변경 모니터링 필요) 한번에 디스크에 내리기 때문에 문제가 커지고 있다는 것을 알 수 있다.

Foreground Eviction이 발생하고 있고, 로그를 먼저 봐서 쿼리 패턴을 보겠죠.

쿼리 성능에 문제가 되는 슬로우 쿼리를 찾아보는 해결책이 있을 수 있다.

LAT 파일 변경이 되고 있구나의 모니터링이 필요하다는 내용을 전달.

→ Overview

---

 

## Replica Set & Sharding

### Replica Set

Primary : 데이터 변경 (default로 Read도 동작)

Secondary : 

가장 큰 장점 : 복제

내부에서 16개의 스레드

→ 복제 지연의 문제는 정말 크게 없었다.

대신 secondary read 

→ 버전 3.x 대 에서는 복제를 반영하는 동안 read 처리를 하지 않는다.

→ 버전 4.x 대 에서는 완화되어서 secondary read의 지연이 없다(전혀 없다라는 게 아니라 완화되었다. 데이터에서는 지연 있긴 하겠죠. 스냅샷을 찍는 주기인 20ms에 복제 지연 초를 더한 값.)

### 정족수

primary, secondary

primary가 죽었을 때 LEFT 알고리즘을 써서 Primary로 만든다.

split brain issue → 동시에 두 개의 primary 가 존재할 수 없음

투표한 애들의 100%가 찬성해야 하고 과반수가 동의하면 primary가 된다.

4대 중에서는 선출을 못한다. (50%가 이상이어야만 함) 그래서 3대끼리 선출하면 됨.

그래서 3대로 레플리카를 설정하는 것보다 5대로 레플리카를 설정하는 것은 고가용성 성능에 아주 큰 차이가 있다.

만약 primary가 2대가 된다면(네트워크 이슈로 죽은 줄 알았을 때) 기존의 primary가 secondary로 내려옴 step down.

hidden 숨어있는 멤버 (secondary 중 hidden은 read를 하지 않음)

`primary secondary secondary (hidden-backup)` 으로 구성했음 (백업용으로 많이 사용하심)

버전업 할 때 hidden 으로 바꾸고 QPT 빠진 걸 보고 버전업하고 다시 올림

설정 중, primary=0 은 절대 primary로 되지 않은 것. vote는 1로 그대로 둘 수 있다.

2번으로는 트래픽이 가지 않음

즉, 정족수에 포함이 되지는 않은데 vote는 할 있다.

### 프라이머리 텀

idle한 환경이라면 10초 이내로 처리됨



---

## 샤딩

몽고 DB의 꽃. 이지만 장애를 일으키는 부분

Mongos

라우터 역할

Config

Shard

여러개의 샤딩을 분리하면 각각의 샤드마다 primary 가 있기 때문에 Write QPS가 높아진다.

저장 공간 측면에서도 역할한다.

mongos 는 메모리 사이즈가 가장 중요함

VM이라도 32GB 정도는 되어야 한다.

Config는 굉장히 중요한… 장애가 많이 발생할 수 있는 부분

리밸런싱 청크 단위로 데이터를 나누는 작업이 필요함

여러 샤드

샤드 키는 변경할 수 없다. 컬렉션 덤푸 후 드랍하고 동일한 컬렉션을 만들어서 리스토어할 수밖에 없음.

샤드 하나라도 문제가 발생했을 때 10개의 서버라면 10개의 서버 전부에게 받아야 쿼리가 진행됨 (어그리게이션 불가능)

_id 로 샤딩 키를 잡을 때 브로드캐스팅 쿼리의 문제가 된다.

가장 중요한 쿼리가 뭐냐할 때 응답이 가장 빨라야 하는 쿼리를 보고 레인지 쿼리로 잡는 것이 낫다.

레인지 샤딩 / 해시샤딩

샤딩 클러스터에서 페이징 쿼리 최대한 안쓰는 게 좋다

각 샤딩 간 청크 단위로 받음

한 건한건 보내고 20만 건을 보내고 → 받으면 insert → 메타데이터 락을 걸고 3번 샤드가 갖고 있다고 변경 후 락 풀음 → 

동일한 샤딩 키는 무조건 청크 하나에 있음.

유니크 인덱스 → 샤딩키 

10개의 샤딩이 있다. 근데 updateOne을 하려는데 해당 데이터에 샤딩키가 없다.

그럼 10개로 질의를 할 수 없기 때문에 바로 에러됨.

## MongoDB Index & Aggregation

MongDB - concern 

local / available / majority / …

unique한 데이터인데 두 개 이상의 샤딩에 있다면(중복될 수 있음) 두 군데에서 결과가 mongos가 받겠죠.

테이블 당 clustered Index는 하나만 있을 수 밖에 없는데 내부에서 이미 쓰고 있기 때문에 사용할 수 없음
