# S3

Section introduction

- Amazon S3 is one of the main building blocks of AWS

-> ”infinitely scaling” storage

- Many websites use Amazon S3 as a backbone

- Many AWS services use Amazon S3 as an integration as well


### Amazon S3 Use cases

- Backup and storage
- Disaster Recovery
- Archive
- Hybrid Cloud storage
- Application hosting
- Media hosting
- Data lakes & big data analytics
- Software delivery
- Static website

- Amazon S3 allows people to store objects (files) in **buckets** (directories)
- Buckets must have a globally unique name (
  -> across all regions all accounts)
- Buckets are defined at the region level
- S3 looks like a global service but buckets are created in a region

## S3 Security

### Access 제한
4가지 방식 존재

**User-Based**
1. IAM Policies: which API calls should be allowed for a specific user from IAM

**Resource-Based**
2. Bucket Policies: bucket wide rules from the S3 console - allows cross account
3. Object Access Control List (ACL): finer grain (can be disabled)
4. Bucket Access Control List (ACL): less common (can be disabled)


- 암호화되지 않은 객체를 처리할 수 있는 방법
  - Batch Operations: Encrypt un-encrypted objects


### Object Encryption

어떻게 S3 데이터를 안전하게 보관할 수 있을까?

4가지 방식 존재

*Server-Side Encryption*

#### 1. SSE-S3
: Server-Side Encryption with Amazon S3-Managed Keys
- 사용자는 Key에 접근 불가한 AWS에 의해 관리되는 방식.
- AWS가 키를 관리하여 S3에 업로드되는 객체들을 모두 암호화.
- 암호화 방식은 AES-256
- 조건: Header에 `"x-amz-server-side-encryption": "AES256"` 반드시 포함

**Process**
1. User가 HTTP(S) Header에 `"x-amz-server-side-encryption": "AES256"`값을 포함한 파일 Request
2. S3 내 File Upload로 Object 생성
3. Object와 S3-Owned-Key를 페어링
4. Mixing 해서 Encryption 진행
5. S3 Bucket에 저장

#### 2. SSE-KMS
: Server-Side Encryption with KMS Keys stored in AWS KMS
- AWS KMS 서비스를 통해 자신의 키를 직접 생성하고 관리할 수 있음
- KMS가 주는 이점: 사용자 Control + 키 감사에 사용할 CloudTrail
  - CloudTrail: KMS에서 키를 사용하면 AWS에서 발생하는 모든 일을 기록하는 역할
- 조건: Header에 `"x-amz-server-side-encryption": "aws:kms"` 반드시 포함

**Process**
1. User가 HTTP(S) Header에 `"x-amz-server-side-encryption": "aws:kms"`값을 포함한 파일 Request
2. S3 내 File Upload로 Object 생성
3. AWS KMS에서 키를 가져와서 Object와 AWS KMS를 페어링
4. Mixing 해서 Encryption 진행
5. S3 Bucket에 저장

**Limitation**
- SSE-KMS를 사용할 경우 GenerateDataKey KMS API를 사용
- 키를 불러올 경우, Decrypt KMS API를 사용
- 이 때 호출하는 KMS API는 KMS 초당 할당량(Quota)에 포함됨
  - 5500, 10000, 30000 req/s (리전 당, based on region)
- 만약, S3 버킷 처리량이 굉장히 높다고 할 때, 모든 객체를 암호화해야 한다면 Throttling 발생 가능


#### 3. SSE-C
: Server-Side Encryption with Customer-Provided Keys
- 암호화 키를 자체적으로 관리하고 싶을 때
- 키는 외부에서 관리하지만, AWS로 키를 보내기 때문에 SSE
- S3 저장하지 않고, 사용 후 폐기
- HTTPS만 지원
- 모든 HTTP 요청 시 HTTP header에 암호화 키 전달해야 함

**Process**
1. User가 **HTTPS** 프로토콜을 통해 Header에 **암호화 키**를 담아 파일 Request
2. S3 내 File Upload로 Object 생성
3. Object와 사용자가 전송한(Client-Provided Key)를 페어링
4. Mixing 해서 Encryption 진행
5. S3 Bucket에 저장

> 해당 파일을 읽기 위해선 암호화한 키 필요

*Client-Side Encryption*

#### 4. Client-Side Encryption**

- **Amazon S3 Client-Side Encryption Library**와 같은 Client 라이브러리 사용
- Sending: S3에 파일을 전송하기 전 반드시 암호화 후 전송
- Retrieving: S3에서 찾은 객체는 암호화되어 있기 때문에 복호화 후 사용 가능
- 사용자(Customer)가 암호화 키와 암호화 과정(encryption cycle)을 전적으로 관리

**Process**
1. User가 Client Key로 File 암호화
2. User가 HTTP(S) 통신으로 암호화한 파일 Request
3. S3 내 File Upload로 Object 생성 후 저장


### 전송 암호화
*: Encryption in transit (SSL/TLS)*

- SSL/TLS 을 통한 전송 암호화 

### Force Encryption:  Bucket Policies
- Deny Access When Requests without Encryption
- 암호화를 강제하는 방법 1: Bucket Policy를 아래와 같이 설정하여 *암호화 헤더가 없는 S3 API 요청을 거절*하는 것

``` json
{
       "Version": "2012-10-17",
       "Id": "PutObjPolicy",
       "Statement": [
           {
                "Sid": "DenyIncorrectEncryptionHeader",
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:PutObject",
                "Resource": "arn:aws:s3:::<bucket_name>/*",
                "Condition": {
                    "StringNotEquals": {
                          "s3:x-amz-server-side-encryption": "aws:kms"
                             }
                   }
           },
           {
                "Sid": "DenyUnEncryptedObjectUploads",
                "Effect": "Deny",
                "Principal": "*",
                "Action": "s3:PutObject",
                "Resource": "arn:aws:s3:::<bucket_name>/*",
                "Condition": {
                    "Null": {
                          "s3:x-amz-server-side-encryption": true
                            }
                    }
           }
    ]
}
```
[AWS Blog: Deny Unencrypted](https://aws.amazon.com/ko/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/)


### Default Encryption

- 암호화를 강제하는 방법 2: S3 **Default Encryption** Option 활성화
  - 암호화 되지 않고 업로드된 객체도 업로드 후 S3에 의해 암호화

***순서: Bucket Policies -> Default Encryption***

[AWS Blog: Default Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)

### CORS

### Access Logs

### Pre-Signed URLs

### Glacier Vault Lock

### Object Lock (versioning must be enabled)

### Access Points

### Object Lambda


## S3 Storage Class
*궁극적 목표: 비용 절감*

- Standard: 가장 보통의 범용 클래스
- Standard IA (Infrequent Access): 자주 사용하지는 않지만 빠르게 접근해야 하는 데이터를 위한 클래스
- One Zone-Infrequent Access: 자주 액세스하지 않지만 필요할 때 빠르게 액세스해야 하는 데이터
- Glacier Storage
  - Glacier Instant Retrieval: 즉각적인 액세스 + 밀리초 단위의 검색 시간
  - Glacier Flexible Retrieval: 즉각적인 액세스 X + 큰 데이터 집합을 검색하는 유연성이 필요한 데이터
  - Glacier Deep Archive: for long term storage. 장기 보관용

### S3 analytics Storage Class Analysis
Lifecycle 을 추천받고 싶다면 S3 Analytics 사용

- 참고: https://docs.aws.amazon.com/ko_kr/AmazonS3/latest/userguide/analytics-storage-class.htmlS3 
- analytics Storage Class Analysis는 적절한 데이터를 적절한 스토리지 클래스에 전송할 수 있도록 스토리지 접근 패턴을 분석 
- 데이터 접근 패턴을 지켜 보며, Standard Class에 있는 객체가 접근이 적어 Standard IA(Infrequent Access)로  이동시킬만 하면, 그 시점을 알려줌으로써 적절한 스토리지를 선택하게끔 도와줌

- Standard나 Standard IA를 사용할 때 추천
- One-Zone IA 나 Glacier에서는 사용하지 못함
- 분석 리포트는 매일 업데이트 (24시간에서 48시간 이후, 분석된 데이터를 확인할 수 있음)
- Lifecycle Rules 사용 초기에 함께 적용하기 좋음
- Lifecycle Rules을 더 개선시킬 때에도 효과적


### Replication
- 버킷 복제는 서로 다른 AWS 계정 간에도 사용할 수 있음
- 복제는 비동기 방식으로 백그라운드에서 이루어짐
- 체이닝 복제는 불가능
- 1번 버킷이 2번 버킷에 복제되어 있고 2번 버킷이 3번 버킷에 복제돼 있다고 해서, 1번 버킷의 객체가 3번 버킷으로 복제되지 않음
  - 1 => 2, 2 => 3 1 =/=>3

#### Cross-Region Replication

: 교차 리전 복제. 어떤 특정한 리전에 S3 버킷이 있고, 이를 다른 리전의 S3 버킷에 복제해야 할 때 사용

**Use cases:**

- CRR: compliance, lower latency access, replication across accounts

#### Same-Region Replication, SRR

: 같은 리전으로 복제. 

쉽게 구분하면, CRR은 이름 그대로 두 리전이 달라야 하며, 반대로 SRR은 같은 리전이어야 합니다

**Use cases:**

- SRR: log aggregation, live replication between production and test accounts


### Spring Batch Replication
: 기존 객체부터 복제에 실패한 객체까지 복제할 수 있는 기능

- 주의: 복제를 활성화한 후에는 새롭게 등록되는 객체만 복제 대상이 됨
- 만약 기존의 객체를 복제하고 싶다면 S3 배치 복제 기능을 사용



### DELETE Operation
삭제하려면 옵션 세팅으로 소스 버킷에서 대상 버킷으로 삭제 마커를 복제할 수 있음

- 버전 ID로 삭제하는 경우 버전 ID는 복제되지 않는데, 이는 영구적인 삭제로 누군가 악의를 품고 한 버킷에서 다른 버킷으로 ID 삭제 마커를 복제하면 안 되기 때문


