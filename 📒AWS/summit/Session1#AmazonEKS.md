# Amazon EKS, 중요한 건 꺾이지 않는 안정성

Q. EKS를 안정성있게 사용할 수 있는 방법?

## Amazon EKS와 안정성이란?
쿠버네티스를 많이 사용: 직접 인프라 운영 -> 트레픽이 많아지면 가용성도 고려

- EKS 소개: 최소한의 리소스로 안정적인 컨테이너 앱 구동, 여러 산업군에서 EKS를 통해 서비스 운영
  - 심리스한 클라우드와 통합
  - 쿠버네티스 업스트림 호환 
  - 오픈 소스 호환


### 컨트롤 플레인
어플리케이션 개발에 집중할 수 있도록 AWS에서 관리한다

### 데이터 플레인
워커 노드의 타입을 결정 

#### EKS에서 추구하고 있는 가치
0. 보안
1. **안정성** -> 오늘의 주제
2. 효율성
3. 운영 자동화
4. 조직 표준화

### 안정성이란? 
: **고가용성** High Availability + **재해복구** Disaster Recovery
천재지변 등 예측하기 어려운 극단적 상황, 목표 (1) 빠른 복구, (2)

### EKS에서의 안정성?
쿠버네티스의 각 컴포넌트가 고장없이 **안정적**이어야 **고가용성** 아키텍처를 만들 수 있다.  -> etcd, node, deploy, svc, vol, pod 

컨트롤 플레인: AWS 책임 관리
데이터 플레인: 고객 책임 관리

즉, 맡고 있는 컨트롤/데이터 플레인을 나눠서 생각할 수 있고, 두 플레인 모두 안정성을 지켜야 하는 것.
**교차 계정 ENI**

**컨트롤 플레인 컴포넌트**
- API Server
- Cloud Controller
- Controller Manager
- Scheduler
- etcd

## 컨트롤 플레인 안정성

**API Server 인스턴스**
API를 통해 클러스터 정보 및 요청을 처리하는 컴토넌트
Active-Active 로드 밸런서 (NLB)
오토 스케일링 그룹의 최소 용향 = 2

**etcd 인스턴스**
클러스터 정보를 저장하는 키-밸류 저장소
오토 스케일링 그룹의 최소 용향 = 3


**컨트롤 플레인 인스턴스 사이즈 스케일링**

- AWS 자체에서 컨트롤 플레인의 성능을 향상 중 (고객 관리가 아니라, 추가비용 지불없이 안정적으로 서비스 운영 가능)

- CPU, 메모리 사용량에 따라 컨트롤 플레인 인스턴스 업/다운



## 데이터 플레인 안정성

**데이터플레인 안정성**
1. 자가 복구: 오류에도 애플리케이션 안정성 유지
2.  영향 범위 최소화: 장애 방생 시에도 중요 워크로드의 안정성 유지
3.  빠른 확장: 대규모 트래픽에도 안정성 유지


**상태관리**
자가 복구하는 법, 상태 관리 메커니즘에 따라 상태 관리 가능

현재 상태 -> YML -> 원하는 상태

**컨트롤 루프**
원하는 상태 spec: ==> status:


**Deployment 정의**
- 일반적으로 deployment 단위로 파드를 구성
- rs -> replica set을 확인하고 수를 확인
- **왜 deployment 단위로 업데이트** : 업데이트 필연적, 어덯게 무중단으로 안정적으로 배포/관리할 수 있을까를 생각해볼 수 있음

## 애플리케이션에 문제가 갱겨도 자가 복구할 수 있을까?
**헬스 체크**
1. HTTP 요청
2. Command
3. TCP Socket: Port 확인


### 1. 자가 복구
- command --5s--> readiness
- **readiness**: 주기적으로 체크해서 애플리케이션이 응답을 받을 상태가 됐는지 확인, 정상인 파드에만 요청을 전달할 수 있도록

- **livenessProbe**: 애플리케이션이 건강한 상태인가? -> 이번에는 컨테이너를 재시작

애플리케이션에 수정, 감지, 배포 다시하는 것까지의 시간이 오래 걸리기 때문에 재시작으로 복구를 하는 것.


### 2. 영향 범위 최소화

스케줄러가 파드를 골고루 배치할 수 있게끔 배치 하지만 우선순위가 있음 -> 가령, node resource, affinity, request/limit
만약, 한 파드에 장애가 났다면, 기본적으로 5분마다 체크하기 때문에 비 정상 상태로 유지될 수 있음
-> 각각의 노드에 파드를 골고루 배치하는 이유임

- **podAntiAffinity**: '가급적 특정 노드에 파드를 배포하지 말아주세요'라는 yml 설정
- **topologySpreadConstraints**: 파드를 분산 설정할 때 푀대 

maxSkew: 1로 10개의 ...


antiAffinity & topologySpreadConstraints 를 통해 ...

## 중요도가 높은 서비스를 안정적으로 유지하려면?

파드의 중요성: 상품에 대한 order / review 서비스가 있을 때,
더 중요한 서비스는 order임
노드 CPU/Memory 부하 발생한다면? pod를 내보냄 (Eviction).
만약, order 서비스가 Eviction이 된다면? 안되겠지.
이를 위해 컨테이너 자원 할당의 limits 설정

서비스 품질 (QoS) 클래스
별도의 클래스 지정이 아닌 Request + Limit의 조합을 통해 자동으로 클래스 지정, 
Guaranteed > Burstable > BestEffort 순으로 자원 사용 보장
limit으로 설정 되지 않은 것을 계속 유지 -> 가장 중요한 서비스를 Guaranteed로 설정해야 한다.


### 3. 빠른 확장성

트래픽으로 인한 부하 급증
쿠버에서는 대규모 트래픽을 확장하기 위해서 
- Horizontal Pod Autoscaler (HPA)
- Cluster Auto Scaler (CA)

Pod Resource, CPU 수집 (Metric Server) <-> HPA Controller -> ReplicaSet -> 새로운 Pod 생성

- 계속 트래픽이 들어오면 Node의 자원이 없어짐
- Pending Pod <-> Cluster Auto Scaler -> Auto Scaling Group -> 새로운 노드에 Pod 생성

**Auto Scaler 사용의 어려움 **
- 빠른 확장이 어려움, 노드 그룹
- 각 서비스마다의 자원이나 cpu 사용률이 다름 -> 노드 그룹을 계속 추가하겠지, Spot도 생성하고,, 너무 많은 요구 사항에 따른 노드 관리가 어려움 --> **Karpenter**

- 단일 프로비저너로 다양한 워크로드 사용가능


- 노드 공간이 있다면 HPA에 의해 파드 추가, 
- 노드가 없다면 Karpenter가 가장 적합한 EC2 인스턴스 컴퓨팅 유형을 선택하여 확장
- 최적화된 용량 제공


## 버전 업데이트 안정성

**EKS 버전 업데이트**
150일 릴리즈/3개월 마이너 업데이트/14개월 지원/ 정도 업데이트 되고 있음
-> 테스트 환경에서 반드시 해봐야겠죠.

In-place 클러스터 버전 업데이트: 업데이트 번거로움을 없애고자 함. 현재 바로 다음 버전으로 업데이트 할 수 있는 ... 
- 현재 서ㄹ치되어 있는 노드들을 순차적으로 업데이트

**업데이트 과정**
- 업데이트 대상이 되는 노드에 cordon(kb command) 명령 입력 (scheduling Disabled) -> Drain 명령으로 하드 없앰 

### pod disruption budget (PDB) 설정

최소한의 파드 수를 유지해줌
만약 drain 명령 시 파드 4개 설정 시, 4개가 설치가 되었을 때 까지 트래픽 막음

**In-Place 우려 사항**
- 이전 버전 롤백 불가능
- 예기치 못한 장애
- 한 단계씩 버전 업데이트

-> 이때는 **Blue/Green 버전 업데이트** (Route53이랑 호환 / 가중치 조절해서 트래픽 조절)
-> 안정성 검증이 확인됐다면 새로운 패치 버전인 Green 클러스터 설정

- 하려면 새로운 클러스터 생성해야하는데, 이거 설정이 너무 번거로움 (휴면 에러 발생 가능)
- cdk - argo,add-ons, git... 각종 클러스터 빌드 배포 환경 설정 (EKS Blueprint)
- 


