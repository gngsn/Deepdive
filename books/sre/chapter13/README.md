# CHAPTER 13. Emergency Response

<i>긴급 대응</i>

<br>

> 살다 보면 무엇이든 깨지기 마련

조직의 장기적인 번영을 위해서는 긴급 상황에 대한 대응이 필수적임.

적절한 대응을 위해 철저한 준비와 긴급 상황과 관련된 정기적인 실습이 필요.

## What to Do When Systems Break

<small><i>시스템 장애 시 대응 방안</i></small>

당황하지 말고, 다른 사람들에게 도움을 요청.

## Test-Induced Emergency

<small><i>사전적 테스트 접근법</i></small>

테스트 접근 방식은 사전적으로 계획되어야 함.

SRE는 시스템에 의도적으로 장애를 유발하여 시스템이 어떻게 실패하는지 관찰하고,
반복되지 않도록 신뢰성을 향상할 수 있는 변경 사항을 적용.

### Details

- 분산 MySQL 데이터베이스 중 하나의 테스트 데이터베이스를 사용해 숨겨진 의존성 확인.
- **테스트 계획** → 수백 대의 데이터베이스 중 하나의 데이터베이스에 대한 접근을 차단.

### Response

- 테스트 시작 후 몇 분 내에 데이터베이스에 의존하는 여러 시스템의 내부 및 외부 사용자로부터 접근 불가 오류가 보고되기 시작.
- 권한 변경을 되돌리려 했으나 작업에 실패했으나, 복제(replica) 및 장애 대비(tailborers) 데이터베이스의 권한을 복구하여 해결.
- 약 한 시간 내 모든 권한이 복구됨.
- 테스트의 영향 범위가 예상보다 넓어, 라이브러리에 대한 신속한 수정이 이루어짐. 이와 유사한 현상이 재발하지 않도록 정기적인 테스트 계획 수립.

### Findings

#### What went well

- 예상대로 테스트의 영향이 컸으나 즉시 종료하여 피해를 최소화함.
- 첫 장애 보고 이후 한 시간 안에 모든 권한을 완벽히 복구함.
- 후속 조치 항목도 신속히 처리되어 동일한 장애에 대비할 수 있게 되었으며, 재발 방지를 위해 정기적인 테스트 계획 수립.

#### What we learned

- 철저한 리뷰를 통해 테스트 범위를 잘 규정했다고 생각했으나, 실제로는 의존 관계 시스템 간 특정 상호작용을 완전히 이해하지 못하고 있었음.
- SRE는 지속적으로 장애 대응 도구와 절차를 테스트 및 재정의하며, 장애 관리 절차가 관련 부서에 명확하게 전달되도록 개선할 필요가 있음.
- 테스트 환경에서 롤백 절차를 테스트하지 않았기 때문에 결함이 발생하여 장애 시간이 더 길어짐. 앞으로는 대규모 테스트 시행 전에 롤백 절차를 완벽히 테스트해야 함.

<br>

## Change-Induced Emergency

변경으로 인한 장애

- 구글은 어마어마한 수의 설정(그것도 엄청 복잡한 설정)을 관리
- 설정 변경으로 인한 서비스 장애 차단을 위해, 변경된 설정이 예상치 못한 결과나 동작이 발생하지 않도록 많은 테스트를 수행
- 구글 인프라스트럭처의 규모와 복잡도 때문에 모든 의존성이나 상호 동작을 일일이 확인하기란 거의 불가능

### Details

서비스의 악의적인 사용을 방지하기 위한 목적으로 인프라스트럭처의 설정을 변경/적용

인프라스트럭처는 기본적으로 외부로 노출되는 모든 서비스들과 함께 동작

→ 변경된 설정은 이런 시스템들에게서 크래시루프(crash-loop)를 유발

### Response

백업용 접근이 가능한 패닉룸(panic-rooms)으로 불리는 방에 모여 설정 롤백을 위해 설정을 변경하여 적용

추후 서비스들이 되살아나기 시작

약 10분 이후, 비상대기 엔지니어가 장애를 선언하고 내부 장애 조치 절차를 따르기 시작

일부 서비스들은 그와 무관한 버그 또는 처음 변경된 설정에 영향을 받아 생성된 잘못된 설정 때문에 한 시간이 지나도 여전히 복구가 되지 않은 상황

### Findings

#### What went well

장애를 단기간으로 가져갈 수 있었던 요인

1. 모니터링 시스템은 바로 문제를 인지하고 알림을 발송
- But 과한 지속적 반복 알림
2. 접근이 불가능한 상황에서도 업데이트를 수행하거나 설정 변경을 롤백할 수 있는 도구 존재
    - 대역외 통신(out-of-band communication) 시스템
        - 일부 더 복잡한 소프트웨어 스택이 사용 불가능했던 상황에서도 모든 사람들이 소통할 수 있음
        - → 고가용적이며 오버헤드가 낮은 백업 시스템을 보유해야 하는지를 잘 보여준 사례
    - 명령줄 도구(command line tools)와 대체 접근 방식 보유
3. 신속한 롤백 조치
    - 설정을 적용한 엔지니어는 사내 접근에 대한 대량의 장애를 인지한 후 곧바로 설정을 롤백

#### What we learned

- 카나리(canary) 테스트에서 설정이 충분히 테스트되지 않았으며, 설정 변경이 위험하다는 인지를 하지 못했음
- 단 몇 분 만에 모든 Region이 오프라인 상태가 되었으므로 엄청난 양의 알림이 발송
    - 장애 조치에 합류한 엔지니어들 사이의 의사소통을 더 어렵게 만드는 요인

구글은 직접 개발한 도구에 대한 의존도가 높음

장애 조치 및 의사소통을 위해 사용하는 소프트웨어 스택의 상당 부분은 장애 당시 크래시루프가 발생한 작업들에 의존하고 있었음

이 장애가 조금이라도 더 길어졌다면 디버깅은 말도 못하게 더뎌졌을 것

<br>

## Process-Induced Emergency

<small><i>절차에 의한 장애</i></small>

신속한 일처리가 반드시 좋은 것만은 아님을 보여주는 예시

### Details

**원인**

1. 자동화 테스트를 진행하던 도중, 사용 종료될 동일한 서버에 두 개의 시스템 종료 요청이 연속적으로 전달
2. 버그로 인해 동일한 설정을 가진 모든 서버에 디스크 삭제 요청이 큐에 기록
3. 모든 서버들의 하드 드라이브가 곧 삭제될 예정

### Response

**✔️장애 대응**

디스크 삭제 요청이 발송되자마자, 

비상대기 엔지니어에게 처음으로 작은 서버가 오프라인 모드로 전환되었다는 호출이 전달되고,

이후, 전 세계의 동일한 종류의 모든 서버들로부터 알림이 발송됨

- 해당 머신들이 모두 제거됨 → 해당 지역으로 오는 트래픽을 다른 지역으로 우회
- 모든 팀의 자동화 시스템을 비활성화 + 추가 자동화와 프로덕션 유지 보수를 일시적으로 중지 또는 중단
- 한 시간 정도 지나자 모든 트래픽이 다른 지역으로 전송
- 요청들은 모두 정상적으로 처리되었고 장애는 공식적으로 조치 완료 처리

**✔️복구**

- 일부 네트워크 링크에서 과부하가 발생 → 병목 구간을 확인한 네트워크 엔지니어들이 트래픽을 줄이도록 조치를 취함. 
- 문제가 발생한 지역 중 한 곳의 서버 설정을 선택해서 서버를 증설.
- 장애 발생 약 세시간 후, 서버 증설 완료 → 다시 사용자들의 요청을 처리하기 시작
- SRE는 서버 재설치 작업을 유기적이지만 수동 절차를 거쳐 진행할 계획을 세움.
  3일쯤 지나자 거의 대부분의 서버들이 복구되었고 나머지는 한두 달 후에 복구.

<br><br>

### Findings

#### What went well

대량의 서버가 영향 받지 않음
기본적으로 대량의 서비 군은 별다른 어려움 없이 극한의 부하 견딜 수 있도록 디자인되어 있음

디스크 종료 요청 처리 매우 빠름. 그사이에 모니터링 시스템 덕분에 신속하게 변경 되돌릴 수 있었음
-> 피해 규모 파악 도움

잘 다져진 장애 관리 프로그램 -> 구성원 장애 대응 절차 신속하게 따름

#### What we learned

근본적인 문제는 시스템 종료 자동화 서버가 전달된 명령 유효성 적절하게 판단하지 못한 것

서버 다시 가동되기 시작했을 때, 시스템 종료 서버는 머신 랙으로부터 빈 응답 받았음

**영(0)이 모든 것 의미**

문제는 빈 응답 걸러내지 못하고 머신데이터베이스에 빈 필터 그대로 전달, 머신데이터베이스가 디스크 삭제 요청 서버에게 모든 머신 디스크 삭제 요청한 것

비상대기 엔지니어는 설치 관련 트래픽 우선 순위 높게 조정함으로써 재설치 문제 해결, 동작 멈춘 머신 재시작 자동화 스크립트 통해 처리

인프라스트럭처가 워커 머신 당 최대 두 개 설치 작업만 실행할 수 있는 제한
+ 파일 전송에 적절하지 않은 QoS 설정
+ 제대로 산정하지 못한 타임아웃 설정

=> 강제 커널 재설치 수행

## All Problems Have Solutions

- 문제 원인이 명확하지 않은 상황도 해결책 존재
- 해결책 생각나지 않으면 다른 사람에게 도움 청해야 함
- '신속하게' 팀 동료 참여하고 도움 요청하고 할 수 있는 모든 것 시도해야 함

시스템 단순히 제대로 동작하지 않을 수 있다는 것뿐만 아니라 예전에는 전혀 상상 못한 방법으로 문제 발생할 수 있다는 경험

## Learn from the Past. Don’t Repeat It.

### Keep a History of Outages

장애 기록 남기기

무언가 배우는데 있어 과거 일 문서로 남기는 것보다 나은 방법 없음
기록은 다른 누군가 실수 바탕으로 학습

광범위하고 솔직하게 작성하되, 무엇보다 중요한 것은 화두 던져야 하는 점

### Ask the Big, Even Improbable, Questions: What If…?

현실보다 나은 테스트 없음

### Encourage Proactive Testing

장애 발생하기 전까지는 시스템과 그 시스템 의존 다른 시스템, 그리고 사용자가 어떻게 반응할지 아무도 모름

## 결론

모두 다른 원인에 의해(첫 번째는 사전 테스트, 두 번째는 설정 변경, 세 번째는 시스템 종료 자동화 때문) 공통점

- 장애 조치하는 사람 우선 침착
- 필요하면 다른 사람에게 도움 요청
- 이전 발생 장애 연구하고 새로운 것 학습
- 조치 취한 사람은 문서화 수행
- 장애 대처하는 사람은 시스템 사전 테스트에 더욱 주의. 테스트 통해 문제 발생 가능성 있는 변경 수정하고 실제 장애 발생하기 전 취약점 발견


---

오역:

```
There were several factors at play that prevented this incident from resulting in a longer-term outage of many of Google’s internal systems.

as is: 이 장애가 구글의 많은 내부 시스템의 장시간의 장애로 이어진데는 몇 가지 요인들이 작용했다.
to be: 수많은 구글의 내부시스템 장애를 장기간으로 끌지 않을 수 있었던 몇 가지 요소가 있다.
```

