# CHAPTER 13. Emergency Response

<i>긴급 대응</i>

<br>

> 살다 보면 무엇이든 깨지기 마련

조직의 장기적인 번영을 위해서는 긴급 상황에 대한 대응이 필수적임.

적절한 대응을 위해 철저한 준비와 긴급 상황과 관련된 정기적인 실습이 필요.

## What to Do When Systems Break

<small><i>시스템 장애 시 대응 방안</i></small>

당황하지 말고, 다른 사람들에게 도움을 요청.

## Test-Induced Emergency

<small><i>사전적 테스트 접근법</i></small>

테스트 접근 방식은 사전적으로 계획되어야 함.

SRE는 시스템에 의도적으로 장애를 유발하여 시스템이 어떻게 실패하는지 관찰하고, 
반복되지 않도록 신뢰성을 향상할 수 있는 변경 사항을 적용.

### Details

- 분산 MySQL 데이터베이스 중 하나의 테스트 데이터베이스를 사용해 숨겨진 의존성 확인.
- **테스트 계획** → 수백 대의 데이터베이스 중 하나의 데이터베이스에 대한 접근을 차단.

### Response

- 테스트 시작 후 몇 분 내에 데이터베이스에 의존하는 여러 시스템의 내부 및 외부 사용자로부터 접근 불가 오류가 보고되기 시작.
- 권한 변경을 되돌리려 했으나 작업에 실패했으나, 복제(replica) 및 장애 대비(tailborers) 데이터베이스의 권한을 복구하여 해결.
- 약 한 시간 내 모든 권한이 복구됨.
- 테스트의 영향 범위가 예상보다 넓어, 라이브러리에 대한 신속한 수정이 이루어짐. 이와 유사한 현상이 재발하지 않도록 정기적인 테스트 계획 수립.

### Findings

#### What went well

- 예상대로 테스트의 영향이 컸으나 즉시 종료하여 피해를 최소화함.
- 첫 장애 보고 이후 한 시간 안에 모든 권한을 완벽히 복구함.
- 후속 조치 항목도 신속히 처리되어 동일한 장애에 대비할 수 있게 되었으며, 재발 방지를 위해 정기적인 테스트 계획 수립.

#### What we learned

- 철저한 리뷰를 통해 테스트 범위를 잘 규정했다고 생각했으나, 실제로는 의존 관계 시스템 간 특정 상호작용을 완전히 이해하지 못하고 있었음.
- SRE는 지속적으로 장애 대응 도구와 절차를 테스트 및 재정의하며, 장애 관리 절차가 관련 부서에 명확하게 전달되도록 개선할 필요가 있음.
- 테스트 환경에서 롤백 절차를 테스트하지 않았기 때문에 결함이 발생하여 장애 시간이 더 길어짐. 앞으로는 대규모 테스트 시행 전에 롤백 절차를 완벽히 테스트해야 함.

<br>

## Change-Induced Emergency

변경으로 인한 장애

- 구글은 어마어마한 수의 설정(그것도 엄청 복잡한 설정)을 관리
- 설정 변경으로 인한 서비스 장애 차단을 위해, 변경된 설정이 예상치 못한 결과나 동작이 발생하지 않도록 많은 테스트를 수행
- 구글 인프라스트럭처의 규모와 복잡도 때문에 모든 의존성이나 상호 동작을 일일이 확인하기란 거의 불가능

### Details

서비스의 악의적인 사용을 방지하기 위한 목적으로 인프라스트럭처의 설정을 변경/적용

인프라스트럭처는 기본적으로 외부로 노출되는 모든 서비스들과 함께 동작

→ 변경된 설정은 이런 시스템들에게서 크래시루프(crash-loop)를 유발

### Response

백업용 접근이 가능한 패닉룸(panic-rooms)으로 불리는 방에 모여 설정 롤백을 위해 설정을 변경하여 적용

추후 서비스들이 되살아나기 시작

약 10분 이후, 비상대기 엔지니어가 장애를 선언하고 내부 장애 조치 절차를 따르기 시작

일부 서비스들은 그와 무관한 버그 또는 처음 변경된 설정에 영향을 받아 생성된 잘못된 설정 때문에 한 시간이 지나도 여전히 복구가 되지 않은 상황

### Findings

#### What went well

장애를 단기간으로 가져갈 수 있었던 요인

1. 모니터링 시스템은 바로 문제를 인지하고 알림을 발송
  - But 과한 지속적 반복 알림
2. 접근이 불가능한 상황에서도 업데이트를 수행하거나 설정 변경을 롤백할 수 있는 도구 존재
   - 대역외 통신(out-of-band communication) 시스템
     - 일부 더 복잡한 소프트웨어 스택이 사용 불가능했던 상황에서도 모든 사람들이 소통할 수 있음
     - → 고가용적이며 오버헤드가 낮은 백업 시스템을 보유해야 하는지를 잘 보여준 사례
   - 명령줄 도구(command line tools)와 대체 접근 방식 보유
3. 신속한 롤백 조치
   - 설정을 적용한 엔지니어는 사내 접근에 대한 대량의 장애를 인지한 후 곧바로 설정을 롤백

#### What we learned

- 카나리(canary) 테스트에서 설정이 충분히 테스트되지 않았으며, 설정 변경이 위험하다는 인지를 하지 못했음
- 단 몇 분 만에 모든 Region이 오프라인 상태가 되었으므로 엄청난 양의 알림이 발송
  - 장애 조치에 합류한 엔지니어들 사이의 의사소통을 더 어렵게 만드는 요인

구글은 직접 개발한 도구에 대한 의존도가 높음

장애 조치 및 의사소통을 위해 사용하는 소프트웨어 스택의 상당 부분은 장애 당시 크래시루프가 발생한 작업들에 의존하고 있었음

이 장애가 조금이라도 더 길어졌다면 디버깅은 말도 못하게 더뎌졌을 것

<br>

## Process-Induced Emergency

<small><i>절차에 의한 장애</i></small>

신속한 일처리가 반드시 좋은 것만은 아님을 보여주는 예시

### Details

자동화 테스트를 진행하던 도중 곧 사용이 종료될 동일한 서버에 두 개의 시스템 종료 요청이 연속적으로 전달

두 번째 시스템 종료 요청에 자동화 시스템의 사소한 버그로 인해 전체 인프라스트럭처에서 동일한 설정으로 설치된 모든 서버에 대한 디스크 삭제 요청이 큐에 기록

모든 서버들의 하드 드라이브가 곧 삭제될 위기

### Response

두 번째 시스템 종료 요청이 발송되자마자, 비상대기 엔지니어에게 첫 번째 작은 서버가 오프라인 모드로 전환되었다는 호출이 전달

- 원인 파악: 해당 머신은 디스크 삭제 큐의 요청 때문에 종료되는 것
- 해당 지역의 트래픽을 다른 지역으로 우회
- 해당 지역의 머신들이 모두 제거되어 그 지역에서 발생한 요청에 응답할 수 없었기 때문
- 해당 지역에서 발생한 요청에 장애가 발생하지 않도록 여력이 있는 다른 지역으로 트래픽을 우회(credirec)
- 전 세계의 동일한 종류의 모든 서버들로부터 일제히 알림이 발송
- 추가 피해가 발생하지 않도록 모든 팀의 자동화 시스템을 비활성화
- 추가 자동화와 프로덕션 유지 보수를 일시적으로 중지 또는 중단
- 한 시간 정도 지나자 모든 트래픽이 다른 지역으로 전송
- 사용자 입장에서는 약간의 지연 응답을 경험
- 요청들은 모두 정상적으로 처리되었고 장애는 공식적으로 조치가 완료

**복구**

- 일부 네트워크 링크에서 과부하가 발생하자 병목 구간을 확인한 네트워크 엔지니어들이 트래픽을 줄이도록 조치를 취함. 문제가 발생한 지역 중 한 곳의 서버 설정을 선택해서 서버를 증설
- 장애 발생 약 세시간 후, 서버 증설 완료
  - 다시 사용자들의 요청을 처리하기 시작
- 미국 팀은 유럽 팀에게 업무를 인계하고 SRE는 서버 재설치 작업을 유기적이지만 수동 절차를 거쳐 진행할 계획을 세움. 당시 그 팀은 세 부분으로 나뉘어 있었으며, 각 부분은 수동 재설치 작업 중 특정한 단계를 담당하기로 함. 3일쯤 지나자 거의 대부분의 서버들이 복구되었고 나머지는 한두 달 후에 복구


<br><br>
