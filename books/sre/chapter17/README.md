# CHAPTER 17. Testing for Reliability

'시스템에 대한 정량화된 신뢰도'는 SRE 엔지니어의 핵심 책임 중 하나

신뢰도는 과거의 신뢰성 정도와 미래의 신뢰성 정도를 모두 포함하여 측정

**과거의 신뢰성**: 
: 모니터링 기록 시스템이 제공하는 분석 데이터를 통해 확보

**미래의 신뢰성**:
: 예측 데이터를 정량화하여 확보

테스트는 변경 사항이 있을 때, 
특정 부분에서 동일한 결과를 기대할 수 있다는 것을 보여주기 위한 메커니즘,

따라서 아래 조건 중 하나는 만족해야 함: 

- 테스트로 인한 소프트웨어나 서버의 변경이 없어야 하며, 이를 통해 향후 시스템 동작이 과거와 유사하게 유지될 수 있어야 함.
- 시스템의 모든 변경 사항에 대해 불확실성을 염두에 두고, 변경된 부분을 명확히 설명할 수 있어야 함.

실제로 수행해야 하는 테스트의 양은 시스템의 신뢰성 요구 수준에 따라 다름

테스트에 의해 검증이 가능한 기반 코드가 늘어날수록 불확실성과 변경으로 인해 발생할 수 있는 잠재적인 신뢰성의 하락을 완화

<pre> Relationships between Testing and Mean Time to Repair 
수정에 소요된 시간(Mean Time to Repair, MTTR): 버그를 수정하기까지 소요된 시간

테스트 시스템을 활용하면 MTTR가 0(Zero)인 버그 확인 가능.
MTTR이 제로라는 것은 시스템 수준의 테스트가 서브시스템에 적용되어,
그 테스트가 모니터링 시스템이 관측할 수 있는 것과 정확히 동일한 문제를 발견해냈다는 것을 의미.
즉, 변경된 코드의 배포 전에 버그를 발견해서 실제 프로덕션 환경에 버그가 있는 코드가 배포될 가능성이 없어짐

제로 MTTR을 통해 더 많은 버그를 찾아낼수록 사용자가 경험하는 장애 발생 시간의 간격(Mean Time Between Failure, MTBF) 이 높아지게 됨
</pre>

## Types of Software Testing

- **Traditional Tests**
  - 전통적인 테스트: 개발하는 동안 소프트웨어가 올바른 동작을 수행하는지 여부를 평가
- **Production Tests**
  - 프로덕션 테스트: 실제 동작하는 웹 서비스에 대해 배포된 소프트웨어 시스템이 올바르게 동작 중인지를 판단하는 테스트

<br>

### Traditional Tests

#### Unit tests

클래스나 함수를 테스트하여 이 단위들로 구성된 전체 소프트웨어 시스템의 동작을 독립적으로 테스트하는 방법

단위 테스트는 함수나 모듈이 시스템이 요구하는 정확한 동작을 수행하는지에 대한 명세(specification)의 역할

테스트 주도 개발(Test Driven Development, TDD) 방법론과 함께 사용

<br>

#### Integration tests

대거(Dagger)와 같은 의존성 주입(dependency injection) 도구는 컴포넌트가 가진 복잡한 의존 객체들의 모조(mock) 객체를 생성해주는 강력한 도구

<pre><b><a href="https://dagger.dev/">Dagger</a></b>
Dagger is a fully static, compile-time dependency injection framework for Java, Kotlin, and Android. It is an adaptation of an earlier version created by Square and now maintained by Google.
</pre>

의존성 주입을 이용하는 가장 보편적인 방법 중 하나는 상태를 저장하는 데이터베이스를 가벼운 Mock 객체로 교체하여 특정 동작을 정확하게 테스트하는 것

<br>

#### System tests

: 종단 간 기능 테스트

<br>

##### Smoke tests

: 스모크 테스트 = 세니티 테스트 (sanity test)

가장 간단한 형태의 시스템 테스트

엔지니어가 시스템의 간단하지만 중요한 동작을 테스트하는 방법

더 많은 범위의 짧은 경로를 테스트하고 비용이 더 필요

<br>

##### Performance tests
: 성능 테스트

개발 과정에서 의존성을 가진 서비스의 응답 시간이나 시스템이 필요로 하는 자원에 대한 요구사항이 완전히 바뀔 수 있으므로 갑자기 시스템의 성능이 느려지는 일이 없다는 것 역시(사용자가 먼저 알아차리기 전에) 확실히 해야 함

어느 정도 시간이 지나면서 시스템의 성능이 줄어들거나 더 많은 자원을 사용하는지를 확인

<br>

##### Regression tests
: 회귀 테스트

이미 알려진 버그들을 통해 시스템의 실패나 잘못된 결과가 나타나는 것을 유추하는 테스트

기존에 시스템 테스트나 통합 테스트 과정에서 발생했던 버그들을 테스트 형태로 작성해두면 엔지니어들이 코드를 리팩토링하는 과정에서 이미 시간과 노력을 들여 발견하고 수정한 버그들을 실수로 다시 만들어내는 일을 방지

**📝 버그 수정에는 시간과 분석에 대한 자원이라는 비용이 소모됨**

- 모든 의존 서비스들(혹은 이들의 Mock 서비스들)과 함께 완전한 형태의 서비스를 이용하여 관련된 테스트를 수행하는 것은 훨씬 더 많은 시간(몇 분에서 몇 시간까지 소요)과 전용의 컴퓨팅 자원이 필요

<br>

### Production Tests

: 프로덕션 테스트 = 블랙박스 테스트

실제 프로덕션 시스템을 대상으로 이루어지는 테스트

<br>

#### Configuration test

구글에서는 웹 서비스의 설정은 파일에 기록되어 버전 제어 시스템에 저장

각 설정 파일마다 별도의 설정 테스트(configuration test)가 구현되어있음

실제 프로덕션 환경에서 동작하는 특정 바이너리가 실제로 어떻게 설정되어있는지를 확인하고 해당 설정 파일과 일치하지 않는 부분을 찾아보는 것

설정 테스트는 버전 제어 시스템에 체크인된 특정 버전의 설정 파일을 이용하여 빌드 및 실행

목표한 버전과 실제로 테스트를 통과한 버전을 비교함으로써 실제 프로덕션 환경이 그동안 진행된 엔지니어링 작업에 비해 얼마나 뒤처져있는지를 판단

<br>

**설정 테스트가 유용한 설정 파일 예시**:

- 파일 콘텐츠를 사용하며, 해당 콘텐츠를 쉽게 조회할 수 있는 실시간 쿼리를 지원
  - 간단하게 쿼리를 실행하여 결과를 실제 파일과 비교

**설정 테스트가 복잡해지는 설정 파일 예시**:

- 바이너리에 내장된 기본 값을 사용하는 설정
  - 테스트의 결과는 별개의 버전이 됨
- 전처리 시 지정되는 설정
  - 배시셸의 명령줄 플래그
- 공유 런타임 중, 특정 문맥에 의존적인 동작을 명시적으로 제어하는 설정
  - 테스트가 런타임 릴리즈 일정에 의존성을 가짐

<br>

#### Stress test

안전한 운영을 위한 시스템의 한계를 알아내기 위해 사용

**검증 예시:**

- 데이터베이스 용량이 어느 정도에 도달할 때 쓰기 작업에 실패하는가?
- 애플리케이션 서버는 초당 몇 개의 쿼리까지 응답할 수 있는가?

<br>

#### Canary test

프로덕션 환경을 대상으로 하는 테스트에는 잘 적용하지 않음

서버의 일부만을 새로운 버전의 바이너리나 설정 파일로 업그레이드한 후 일정 기간 동안 지켜보는 형태로 진행

- 문제가 발생하지 않으면 → 나머지 서버들도 점진적으로 업그레이드
- 문제가 발생하면 → 문제가 발생한 해당 서버만 롤백

업그레이드된 서버들을 지켜보는 기간을 '**baking the binary**'라고 표현함

- **설정 테스트** / **스트레스 테스트**: 지정된 소프트웨어에 대한 특정 상태의 존재 여부를 파악하기 위한 방법인 반면, 
- **카나리 테스트**: 그보다 더 즉흥적으로 이루어지는 방법임

<br>

> #### 예시
>
> 사용자 트래픽에 상대적으로 영향을 미친 결함이 업그레이드를 거쳐 배포된 후 갑자기 기하급수적으로 증가함.
>
> **오류의 누적 횟수 CU=PK**
>
> - R: the rate of those reports. 리포트의 비율
> - U: the order of the fault (defined later). 오류 순서
> - K: the period over which the traffic grows by a factor of e, or 172%. 특정 요인 e에 의해 트래픽이 증가된 기간 또는 172%를 의미
>
> 사용자의 피해를 줄이려면 예상하지 못한 문제가 발생한 배포를 신속하게 취소하고 이전 버전의 설정으로 롤백해야 함
>
> 
> 잠시 동안 자동화를 이용해 변화의 추이와 반응을 살펴봄.
> 
> 그 동안 몇 개의 추가 보고가 생성됨.
> 
> 일단 문제가 해결되면 이 보고들을 통해 누적 횟수 C와 비율 R을 예측할 수 있음.
>
> 
> 이를 X로 나누면 발생했던 문제의 순서인 U를 예측할 수 있음.
>
> - `U=1`: 사용자의 요청이 문제가 발생한 코드로 인해 처리되지 않음
> - `U=2`: 사용자의 요청이 랜덤하게 데이터를 손실시켜 다음 사용자의 요청에서 손실된 데이터가 나타남
> - `U=3`: 랜덤하게 손실된 데이터가 이전 요청에서 유효한 식별자로 사용됨
>
> 대부분의 버그는 첫 번째 해당 <- 사용자 트래픽의 양에 따라 선형적으로 증가
>
> 높은 순서의 버그와 낮은 순서의 버그의 우선순위를 고려한다면, 지수적인 롤아웃 전략을 사용할 때, 사용자 트래픽과의 연관 관계를 생각하지 않아도 됨.
>
> 동일한 K 간격을 사용하는 한, 오류 원인을 확정할 수 없더라도, U에 대한 예측값은 여전히 유효하기 때문.
>
> 
> 약간의 중복 값을 허용하면서 순차적으로 많은 방법을 사용하면 X의 값을 작게 유지할 수 있음.
> 
> U의 값을 조기 예측하면서, 사용자가 장애를 경험하는 전체 횟수인 C를 최소화할 수 있음.

<br>

## Creating a Test and Build Environment

<i><small>테스트 및 빌드 환경 구성하기</small></i>

팀의 기반 코드가 여전히 프로토타입 단계이며, 이에 대한 전반적인 테스트는 아직 정의되지 않은 상태인 경우임. 이런 상황이라면 어느 부분부터 테스트를 시작해야 할까? 

최소한의 노력으로 최상의 효과를 낼 수 있는 테스트를 수행해야 함

- 코드의 우선순위를 결정할 수 있는가?
- 비즈니스 관점에서 중요한 기능이나 클래스를 특정할 수 있는가?
- 다른 팀들이 통합해서 사용하는 API들이 있는가? 

강력한 테스트 문화를 수립할 수 있는 방법 중 하나는 지금까지 보고된 모든 버그를 **테스트 케이스의 형태로 문서화**하는 것임

Bazel은 소프트웨어 프로젝트의존성 그래프를 생성해줌. 어떤 파일에 변경이 발생하면 Bazel은 그 파일에 의존하는 부분만 다시 빌드함

또한 이런 시스템들은 재생산 가능한 빌드를 제공함

<br>

## Testing at Scale

<i><small>대규모 환경에서의 테스트</small></i>

상대적으로 규모가 작은 단위 테스트의 경우 몇 가지 의존성을 가지고 있음
- 단일 소스 파일, 테스트 라이브러리, 런타임 라이브러리, 컴파일러, 테스트를 실행하는 로컬 하드웨어 등

견고한 테스트 환경이라면 이들 역시 테스트 환경의 각기 다른 부분들에 대해 기대하는 여러 가지 활용 사례(use case)를 확인하기 위한 자신만의 테스트 커버리지를 확보하고 있어야 함

실용적인 테스트 환경은 여러 버전과 병합(merge) 중에서 적절한 브랜치를 선택해 테스트를 수행할 수 있어야 함

(각 브랜치에 해당 기능을 테스트할 수 있도록 코드를 잘 분리해야 함)

<br>

### Testing Scalable Tools

<i><small>대규모 환경을 위한 도구의 테스트</small></i>

**SRE가 사용하는 도구의 테스트**

**수행 작업:**
- 데이터베이스 성능 지표의 조회 및 배포
- 가용성 위험에 대한 계획 수립을 위한 사용량 예측
- 사용자가 접근할 수 없는 서비스 복제본의 데이터 리팩토링
- 서버상의 파일 변경

**특징:**
- 도구들의 부작용이 테스트를 수행한 메인스트림 API에 그대로 남음
- SRE가 개발한 도구들은 유효성 검사 및 릴리즈 장벽으로 인해 사용자 직접 접근하는 프로덕션 환경과는 격리된 환경에서 실행

**자동화 도구 테스트**

훨씬 세밀한 테스트가 필요함. 자동화 도구들은 다음과 같은 작업을 수행함

- 데이터베이스 인덱스 선택
- 데이터센터 간의 로드 밸런싱
- 신속한 리마스터링(remastering)을 위한 릴레이 로그 혼합

**특징:**
- 실제로 작업은 주로 견고하고, 예측 가능하며, 충분히 테스트된 API들을 대상으로 수행됨
- 수행되는 작업의 목적에 따라 작업 수행 도중 다른 API 클라이언트들에게서 알 수 없는 중단 현상이 발생할 수 있음

한 자동화 도구가 다른 자동화 도구가 실행되는 환경을 바꿔버릴 수 있다는 점 고려해야 함

<br>

### Testing Disaster

<i><small>재해 테스트</small></i>

많은 재해 복구 도구는 오프라인 상태에서도 동작할 수 있도록 세심한 주의를 기울여 만들어짐

- 깨끗하게 정지시킨 서비스 상태가 되도록 체크포인트(checkpoint) 계산
- 기존의 무결성 도구를 적재할 수 있도록, 체크포인트를 적재 가능한 상태로 만듦
- 재시작 절차를 수행하는 릴리즈 경계 도구들을 지원

온라인 복구 도구들은 본질적으로 메인스트림 API와는 별개로 동작함

최종적 일관성(eventual consistency)을 지향하는 본질적인 성향이 복구 과정에는 오히려 도움이 되지 않는 경우가 있음

<br>

### The Need for Speed

<i><small>지금 필요한 것은 스피드</small></i>

코드 저장소의 모든 버전(패치)에 대해 정의된 모든 테스트들은 성공 혹은 실패만을 표시할 뿐

시나리오에 대한 가설을 완성하고 적절한 버전의 코드에 대해 적당한 횟수의 테스트를 반복해서 실행함으로써 합당한 추론을 이끌어낼 수 있음

새로 작업한 패치를 테스트하기 위해서는, 패치를 적용하기 전의 기반 코드에 대한 테스트 수행 결과와 패치를 적용한 이후의 기반 코드에 대한 테스트 수행 결과를 비교함

대부분의 프로덕션 환경 관리는 소스 제어 저장소에 보관되지만 설정값 자체는 개발자의 소스 코드와는 별개로 존재

<br>

### Pushing to Production

<i><small>프로덕션 환경에 배포하기</small></i>

SRE 모델에서는, 테스트 인프라를 프로덕션 설정에서 분리하면 프로덕션 환경을 묘사하는 모델과 애플리케이션 동작을 묘사하는 모델이 일치하지 않으므로 상당히 좋지 않음

테스트 시스템이 10번 중 한번 정도 잘못된 결과를 리턴하는 것을 용인할 수 있겠지만, 테스트 커버리지의 일부는 다른 부분에 비해 현저히 높은 수준을 요구

<br>

### Expect Testing Fail

<i><small>테스트는 얼마든지 실패할 수 있다</small></i>

장애 발생 시간 간격(Mean Time Between Failure, MTBF)

신뢰성에 대한 세심한 관리란 릴리즈 주기를 위해 사용자가 인지할 수 있는 문제의 해결을 어느 정도까지 제한할 것인지와 불확실성을 최소화할 수 있는 테스트 커버리지의 범위를 어느 정도까지 제한할 것인지를 모두 고려하여 관리하는 것

신뢰성을 바탕으로 릴리즈 주기를 정의한다면 기능별로, 혹은 팀별로 신뢰성의 수준을 나누는 편이 나음

이런 시나리오에서 기능 엔지니어링 팀은 목표 릴리즈 주기를 맞추기 위한 기준이 되는 불확실성 수준을 달성하는 것을 목표로 삼고, SRE 팀은 그들과 관련된 불확실성 수준을 별도로 정의해서 릴리즈 비율을 극대화하기 위해 노력해야 함

적절한 수준의 신뢰성을 유지하고 서비스를 지원하는 SRE의 수가 기하급수적으로 증가하는 상황을 방지하기 위해서는 프로덕션 환경이 거의 자동화되어야 함. 그러려면 환경 스스로가 경미한 장애에 대해서는 탄력적으로 대응할 수 있어야 함. SRE의 개입이 필요한 큰 장애가 발생하면, SRE들은 제대로 테스트가 된 도구들을 사용해야 함. 그렇지 않으면, 기록된 데이터를 바탕으로 향후의 장애를 해결할 수 있을지에 대해 충분한 자신감을 갖기 어려울 것

설정 파일이 존재하는 주된 이유는 도구를 새로 만드는 것보다는 설정을 바꾸는 편이 훨씬 빠르기 때문
이를 토대로 MTTR을 낮게 유지할 수 있음

- MTTR을 낮게 유지하기 위해 필요하며, 장애가 발생한 경우에만 변경되는 설정 파일은 MTBF보다 느린 릴리즈 주기를 가져야 함
- 설정 파일에 대한 테스트 및 모니터링의 커버리지가 사용자 애플리케이션의 테스트 및 모니터링 커버리지보다 뛰어나지 않다면 이 파일은 사이트의 신뢰성에 악영향을 미칠 수 있음

- 각 설정 파일은 수정에 대해 충분한 테스트 커버리지를 확보해야 함
- 릴리즈에 앞서 릴리즈 테스트를 수행한 후에 파일의 수정이 이루어져야 함
- 유리 깨기 메커니즘(break-glass mechanism)을 통해 테스트가 완료되기 전에 파일을 밀어 넣을 수 있는 방법을 제공해야 함. 이 방법은 신뢰성을 떨어뜨릴 수 있으므로 나중을 위해 발견된 버그에 대해서는 문제를 크게 부풀려서 더욱 확실하게 해결할 수 있도록 하는 것이 좋음.

<br>

### Integration

<i><small>통합</small></i>

단위 테스트를 통해 설정 파일의 신뢰성을 확보하는 것과 더불어 설정 파일에 대한 통합 테스트를 고려하는 것 역시 중요

설정 파일을 인터프리터 언어를 통해 작성하는 것은 잠재적인 실패를 완전하게 처리하기가 어렵기 때문에 위험이 따름

프로토콜 버퍼(protocol buffers)를 사용할 때의 장점은 스키마를 미리 정의하고 읽는 시점에 자동으로 체크할 수 있는데, 사용하기가 쉬우면서도 특정 런타임에 한정적으로 사용할 수 있다는 점

SRE의 역할에는 시스템 엔지니어링 도구를 작성하고 필요하다면 테스트 커버리지에 더 견고한 유효성 검사를 추가하는 것도 포함됨
엔지니어는 다른 도구들이 정상적으로 동작하는지 확인하고 이를 이용해 앞서 발생한 오동작을 최대한 보완할 수 있어야 함

<br>

### Production Probes

<i><small>프로덕션 환경 조사하기</small></i>

- 테스트: 알려진 데이터에 대한 시스템의 허용 가능한 동작을 확인
- 모니터링: 알려지지 않은 사용자 데이터에 대한 동작을 검증

문제가 있는 요청에 대해서는 에러가 발생해야 하고 문제가 없는 요청은 반드시 처리되어야 함

- 릴리즈 테스트는 보통 프런트 엔드와 mock 백엔드로 구성된 봉합 서버를 대상으로 실행
- 조사 테스트는 릴리즈 바이너리와 로드 밸런싱 프런트엔드, 그리고 별개의 영속 백엔드를 토대로 실행됨
- 프런트엔드와 백엔드는 서로 독립적으로 릴리즈

테스트 요청 종류:

- 알려진 잘못된 요청 (에러를 발생시켜야 함)
- 프로덕션에서 재실행 가능한 알려진 올바른 요청
- 프로덕션에서 재실행 불가능한 알려진 올바른 요청

프로브가 실패할 경우:
프로덕션 모니터링 프로브가 실패하면 다음 중 하나가 문제일 가능성이 높음:

- 프런트엔드 API와 백엔드 API가 프로덕션과 릴리즈 환경에서 일치하지 않음
- 프런트엔드와 백엔드의 릴리즈 주기가 다르기 때문에 환경이 다르게 구성됨

프로덕션 업데이트는 모든 조합(새 버전 ↔ 기존 버전)을 테스트하며, 문제가 발생하면 자동으로 롤백함

이런 자동화된 모니터링과 테스트는 빠른 피드백을 제공하여 문제 해결 시간을 단축

- **프로브 테스트**: 프로덕션과 릴리즈 환경의 불일치를 조기에 발견
- **자동화된 업데이트**: 새로운 버전에서 발생할 수 있는 문제를 조합별로 테스트하여 빠르게 롤백 가능

<br>

## Conclusion

<i><small>결론</small></i>

테스트는 엔지니어들이 자신의 제품의 신뢰성을 향상시킬 수 있는 가장 적절한 투자 수단

